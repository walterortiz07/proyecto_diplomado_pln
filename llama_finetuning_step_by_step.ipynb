{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc15fca",
   "metadata": {},
   "source": [
    "# Fine-tuning paso a paso: Llama-3.2-1B con AmericasNLP y diccionario estructurado\n",
    "\n",
    "Este notebook guía el proceso completo y reproducible para ajustar un modelo LLM (meta-llama/Llama-3.2-1B) usando HuggingFace Transformers, con datos de AmericasNLP y un diccionario estructurado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c45c23",
   "metadata": {},
   "source": [
    "## 1. Importar librerías necesarias\n",
    "\n",
    "Instala y carga todas las librerías requeridas para el fine-tuning reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8cf15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ejecutable: c:\\Users\\walte\\miniconda3\\envs\\python_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python ejecutable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18076a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.38.2\n",
      "Uninstalling transformers-4.38.2:\n",
      "  Successfully uninstalled transformers-4.38.2\n",
      "Found existing installation: accelerate 0.27.2\n",
      "Uninstalling accelerate-0.27.2:\n",
      "  Successfully uninstalled accelerate-0.27.2\n",
      "Found existing installation: peft 0.9.0\n",
      "Uninstalling peft-0.9.0:\n",
      "  Successfully uninstalled peft-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping sentence-transformers as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.9.1\n",
      "Uninstalling torch-2.9.1:\n",
      "  Successfully uninstalled torch-2.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\~.rch'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Skipping torchvision as it is not installed.\n",
      "WARNING: Skipping torchaudio as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y transformers accelerate peft sentence-transformers\n",
    "%pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7266cb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.38.2\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Collecting accelerate==0.27.2\n",
      "  Using cached accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting peft==0.9.0\n",
      "  Using cached peft-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: datasets in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from transformers==4.38.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from accelerate==0.27.2) (5.9.0)\n",
      "Collecting torch>=1.10.0 (from accelerate==0.27.2)\n",
      "  Using cached torch-2.9.1-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from requests->transformers==4.38.2) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from requests->transformers==4.38.2) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch>=1.10.0->accelerate==0.27.2) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch>=1.10.0->accelerate==0.27.2) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.27.2) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from tqdm>=4.27->transformers==4.38.2) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "Using cached peft-0.9.0-py3-none-any.whl (190 kB)\n",
      "Using cached torch-2.9.1-cp312-cp312-win_amd64.whl (110.9 MB)\n",
      "Installing collected packages: torch, accelerate, transformers, peft\n",
      "\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------------------------------------- 0/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   ---------- ----------------------------- 1/4 [accelerate]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [peft]\n",
      "   ------------------------------ --------- 3/4 [peft]\n",
      "   ------------------------------ --------- 3/4 [peft]\n",
      "   ---------------------------------------- 4/4 [peft]\n",
      "\n",
      "Successfully installed accelerate-0.27.2 peft-0.9.0 torch-2.9.1 transformers-4.38.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.2.0+cu118\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp312-cp312-win_amd64.whl (2704.2 MB)\n",
      "Collecting torchvision==0.17.1+cu118\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.17.1%2Bcu118-cp312-cp312-win_amd64.whl (4.9 MB)\n",
      "Collecting torchaudio==2.2.0\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp312-cp312-win_amd64.whl (3.9 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch==2.2.0+cu118) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch==2.2.0+cu118) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch==2.2.0+cu118) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch==2.2.0+cu118) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch==2.2.0+cu118) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torch==2.2.0+cu118) (2025.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\walte\\miniconda3\\envs\\python_env\\lib\\site-packages (from torchvision==0.17.1+cu118) (1.26.4)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested torch==2.2.0+cu118\n",
      "    torchvision 0.17.1+cu118 depends on torch==2.2.1+cu118\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install torch==2.2.0+cu118 and torchvision==0.17.1+cu118 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.38.2 accelerate==0.27.2 peft==0.9.0 datasets\n",
    "%pip install torch==2.2.0+cu118 torchvision==0.17.1+cu118 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c45f62",
   "metadata": {},
   "source": [
    "Reiniciar Kernels:\n",
    "Ctrl + Shift + P\n",
    "Jupyter: Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb7eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W1214 22:25:22.843000 42400 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.38.2\n",
      "Accelerate: 0.27.2\n",
      "PEFT: 0.9.0\n",
      "Torch: 2.9.1+cpu\n",
      "✔ Versiones correctas\n"
     ]
    }
   ],
   "source": [
    "import transformers, accelerate, peft, torch\n",
    "\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "assert transformers.__version__ == \"4.38.2\"\n",
    "assert accelerate.__version__ == \"0.27.2\"\n",
    "assert peft.__version__ == \"0.9.0\"\n",
    "\n",
    "print(\"✔ Versiones correctas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31115902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Imports OK\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "print(\"✔ Imports OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abee452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ CUDA no disponible\n",
      "✔ Semilla fijada\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"⚠️ CUDA no disponible\")\n",
    "\n",
    "print(\"✔ Semilla fijada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf0f728",
   "metadata": {},
   "source": [
    "## 2. Cargar y preparar el conjunto de datos\n",
    "\n",
    "Cargamos los archivos de AmericasNLP y el diccionario estructurado, y preparamos los pares (input, output) para el fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd52eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (178, 4) | Dev: (79, 4) | Test: (364, 4)\n",
      "Diccionario guaraní-español: (12073, 4)\n",
      "Diccionario español-guaraní: (15891, 4)\n",
      "Ejemplo guaraní-español:\n",
      "  palabra_guarani tipo_etiqueta categoria_gramatical  \\\n",
      "0               a           NaN                  NaN   \n",
      "1               a           NaN          f. v. ra s.   \n",
      "2               ã           NaN                 m sa   \n",
      "3               ã           NaN            adj. dem.   \n",
      "4               ã           NaN                ta s.   \n",
      "\n",
      "                                   traduccion_limpia  \n",
      "0        Vocal que se pronuncia igual que en español  \n",
      "1      Pref. a. de 1ª. Per. sin. para verbos propios  \n",
      "2  Vocal que se pronuncia igual que la a pero con...  \n",
      "3                                              Estos  \n",
      "4                                              estas  \n",
      "Ejemplo español-guaraní:\n",
      "  palabra_espanol tipo_etiqueta categoria_gramatical  \\\n",
      "0               a           NaN          f. va m va.   \n",
      "1               a           NaN                prep.   \n",
      "2               a           NaN                    m   \n",
      "3               a           NaN                  NaN   \n",
      "4               a           NaN                  NaN   \n",
      "\n",
      "           traduccion_limpia  \n",
      "0  Pu’ae avañe’ẽmeicha ipúva  \n",
      "1                         Pe  \n",
      "2                          e  \n",
      "3                         ve  \n",
      "4                         re  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar AmericasNLP (train/dev/test)\n",
    "train_path = 'AmericasNLP/guarani-train.tsv'\n",
    "dev_path = 'AmericasNLP/guarani-dev.tsv'\n",
    "test_path = 'AmericasNLP/guarani-test.tsv'\n",
    "\n",
    "try:\n",
    "    df_train = pd.read_csv(train_path, sep='\\t')\n",
    "    df_dev = pd.read_csv(dev_path, sep='\\t')\n",
    "    df_test = pd.read_csv(test_path, sep='\\t')\n",
    "    print('Train:', df_train.shape, '| Dev:', df_dev.shape, '| Test:', df_test.shape)\n",
    "except Exception as e:\n",
    "    print('Error cargando AmericasNLP:', e)\n",
    "\n",
    "# Cargar diccionarios estructurados de la carpeta archivos\n",
    "dict_ge_path = 'archivos/diccionario_guarani_espanol_estructurado.tsv'\n",
    "dict_eg_path = 'archivos/diccionario_espanol_guarani_estructurado.tsv'\n",
    "\n",
    "try:\n",
    "    df_dict_ge = pd.read_csv(dict_ge_path, sep='\\t')\n",
    "    print('Diccionario guaraní-español:', df_dict_ge.shape)\n",
    "except Exception as e:\n",
    "    print('Error cargando diccionario guaraní-español:', e)\n",
    "\n",
    "try:\n",
    "    df_dict_eg = pd.read_csv(dict_eg_path, sep='\\t')\n",
    "    print('Diccionario español-guaraní:', df_dict_eg.shape)\n",
    "except Exception as e:\n",
    "    print('Error cargando diccionario español-guaraní:', e)\n",
    "\n",
    "# Ejemplo de los primeros registros de cada diccionario\n",
    "print('Ejemplo guaraní-español:')\n",
    "print(df_dict_ge.head())\n",
    "print('Ejemplo español-guaraní:')\n",
    "print(df_dict_eg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1060184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo español-guaraní estructurado:\n",
      "  input                     output\n",
      "0     a  Pu’ae avañe’ẽmeicha ipúva\n",
      "1     a                         Pe\n",
      "2     a                          e\n",
      "3     a                         ve\n",
      "4     a                         re\n",
      "Ejemplo guaraní-español estructurado:\n",
      "  input                                             output\n",
      "0     a        Vocal que se pronuncia igual que en español\n",
      "1     a      Pref. a. de 1ª. Per. sin. para verbos propios\n",
      "2     ã  Vocal que se pronuncia igual que la a pero con...\n",
      "3     ã                                              Estos\n",
      "4     ã                                              estas\n"
     ]
    }
   ],
   "source": [
    "# Preprocesar los diccionarios estructurados para tareas de traducción\n",
    "if 'df_dict_eg' in locals():\n",
    "    dict_data_eg = df_dict_eg[['palabra_espanol', 'traduccion_limpia']].copy()\n",
    "    dict_data_eg = dict_data_eg.rename(columns={'palabra_espanol': 'input', 'traduccion_limpia': 'output'})\n",
    "    print('Ejemplo español-guaraní estructurado:')\n",
    "    print(dict_data_eg.head())\n",
    "else:\n",
    "    print('No se cargó el diccionario español-guaraní.')\n",
    "\n",
    "if 'df_dict_ge' in locals():\n",
    "    dict_data_ge = df_dict_ge[['palabra_guarani', 'traduccion_limpia']].copy()\n",
    "    dict_data_ge = dict_data_ge.rename(columns={'palabra_guarani': 'input', 'traduccion_limpia': 'output'})\n",
    "    print('Ejemplo guaraní-español estructurado:')\n",
    "    print(dict_data_ge.head())\n",
    "else:\n",
    "    print('No se cargó el diccionario guaraní-español.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756bb2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Ha’e ombojerekuri umi kutuhakuéra poro’o [PERS...   \n",
      "1  Ha’e ombojerekuri umi kutuhakuéra poro’o [TYPE...   \n",
      "2  Ha’e ombojerekuri umi kutuhakuéra poro’o [ASPE...   \n",
      "3    Mombe’ukuéra omboty kuri pende arete [TYPE:NEG]   \n",
      "4  Mombe’ukuéra omboty kuri pende arete [ASPECT:I...   \n",
      "\n",
      "                                            output  \n",
      "0       Nde rembojerekuri umi kutuhakuéra tuicháva  \n",
      "1    Ha’e ndombojereikuri umi kutuhakuéra tuicháva  \n",
      "2  Ha’e ombojerehina kuri umi kutuhakuéra tuicháva  \n",
      "3          Mombe’ukuéra ndombotyi kuri pende arete  \n",
      "4        Mombe’ukuéra omboty kuri hína pende arete  \n"
     ]
    }
   ],
   "source": [
    "# Preprocesar AmericasNLP: crear pares (input, output) para fine-tuning\n",
    "\n",
    "def build_input(row):\n",
    "    return f\"{row['Source']} [{row['Change']}]\"\n",
    "\n",
    "df_train = df_train.dropna(subset=['Source', 'Change', 'Target'])\n",
    "df_dev = df_dev.dropna(subset=['Source', 'Change', 'Target'])\n",
    "\n",
    "train_data = pd.DataFrame({\n",
    "    'input': df_train.apply(build_input, axis=1),\n",
    "    'output': df_train['Target']\n",
    "})\n",
    "dev_data = pd.DataFrame({\n",
    "    'input': df_dev.apply(build_input, axis=1),\n",
    "    'output': df_dev['Target']\n",
    "})\n",
    "\n",
    "# Concatenar los datos de los diccionarios estructurados para aumentar el set de entrenamiento\n",
    "train_data = pd.concat([train_data, dict_data_eg, dict_data_ge], ignore_index=True)\n",
    "\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7388c3",
   "metadata": {},
   "source": [
    "## 3. Tokenización de los datos\n",
    "\n",
    "Utilizamos el tokenizer del modelo base para convertir los textos en tensores adecuados para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6729a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 28142/28142 [00:01<00:00, 17225.66 examples/s]\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 8380.97 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 28142\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 79\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar el tokenizer de Bloom-560m desde HuggingFace\n",
    "model_name = 'bigscience/bloom-560m'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenización de los datos de entrenamiento y validación\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['input'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Convertir a HuggingFace Dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_hf = Dataset.from_pandas(train_data)\n",
    "dev_hf = Dataset.from_pandas(dev_data)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_hf,\n",
    "    'validation': dev_hf\n",
    "})\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054462f",
   "metadata": {},
   "source": [
    "## 4. Configurar el modelo base para fine tuning\n",
    "\n",
    "Cargamos el modelo preentrenado Llama-3.2-1B y lo preparamos para el ajuste fino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b781d01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado: bigscience/bloom-560m\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo base para causal language modeling (Bloom-560m)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print('Modelo cargado:', model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c83be",
   "metadata": {},
   "source": [
    "## 5. Definir hiperparámetros y configuración de entrenamiento\n",
    "\n",
    "Establecemos los parámetros principales para el entrenamiento reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75e41dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparámetros definidos.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./finetuned',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to='none',\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "print('Hiperparámetros definidos.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47687c",
   "metadata": {},
   "source": [
    "## 6. Entrenar el modelo\n",
    "\n",
    "Ejecutamos el proceso de entrenamiento usando los datos tokenizados y la configuración definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36105bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    acc = np.mean([\n",
    "        p.strip() == l.strip()\n",
    "        for p, l in zip(decoded_preds, decoded_labels)\n",
    "    ])\n",
    "\n",
    "    return {\"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc54951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "dev_dataset = Dataset.from_pandas(dev_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d0b1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60256fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28142/28142 [00:02<00:00, 9463.98 examples/s] \n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 6002.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "dev_dataset = dev_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d04f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c1fa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10554 [00:00<?, ?it/s]c:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "  0%|          | 9/10554 [02:55<53:18:06, 18.20s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1622\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[32m   1625\u001b[39m         args=args,\n\u001b[32m   1626\u001b[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001b[32m   1627\u001b[39m         trial=trial,\n\u001b[32m   1628\u001b[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001b[32m   1629\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\transformers\\trainer.py:1961\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   1958\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   1960\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m1961\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs)\n\u001b[32m   1963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1964\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   1965\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[32m   1966\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   1967\u001b[39m ):\n\u001b[32m   1968\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   1969\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\transformers\\trainer.py:2902\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   2899\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   2901\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m2902\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.compute_loss(model, inputs)\n\u001b[32m   2904\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.n_gpu > \u001b[32m1\u001b[39m:\n\u001b[32m   2905\u001b[39m     loss = loss.mean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\transformers\\trainer.py:2925\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs)\u001b[39m\n\u001b[32m   2923\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2924\u001b[39m     labels = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2925\u001b[39m outputs = model(**inputs)\n\u001b[32m   2926\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   2927\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\transformers\\models\\bloom\\modeling_bloom.py:871\u001b[39m, in \u001b[36mBloomForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[39m\n\u001b[32m    858\u001b[39m transformer_outputs = \u001b[38;5;28mself\u001b[39m.transformer(\n\u001b[32m    859\u001b[39m     input_ids,\n\u001b[32m    860\u001b[39m     past_key_values=past_key_values,\n\u001b[32m   (...)\u001b[39m\u001b[32m    867\u001b[39m     return_dict=return_dict,\n\u001b[32m    868\u001b[39m )\n\u001b[32m    869\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states)\n\u001b[32m    873\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    874\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    875\u001b[39m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\walte\\miniconda3\\envs\\python_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d2f80",
   "metadata": {},
   "source": [
    "## 7. Evaluar el modelo ajustado\n",
    "\n",
    "Evaluamos el modelo fine-tuned en el conjunto de validación y mostramos métricas relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de validación\n",
    "eval_results = trainer.evaluate()\n",
    "print('Resultados de evaluación:', eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174464c",
   "metadata": {},
   "source": [
    "## 8. Guardar el modelo fine-tuned\n",
    "\n",
    "Guardamos el modelo entrenado y el tokenizer para su uso posterior o despliegue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7157bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo y el tokenizer\n",
    "output_dir = './llama3-finetuned-final'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f'Modelo y tokenizer guardados en {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ea1b2",
   "metadata": {},
   "source": [
    "## 9. Inferencia: probar el modelo fine-tuned\n",
    "\n",
    "Ejecuta el modelo ajustado sobre ejemplos nuevos o del set de test, como se requiere en la consigna del trabajo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de inferencia con el modelo fine-tuned\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar modelo y tokenizer fine-tuned\n",
    "dir_finetuned = './llama3-finetuned-final'\n",
    "model = AutoModelForCausalLM.from_pretrained(dir_finetuned)\n",
    "tokenizer = AutoTokenizer.from_pretrained(dir_finetuned)\n",
    "\n",
    "# Ejemplo: usar un input del set de test o uno propio\n",
    "test_example = \"Ore ndorombyai kuri [TYPE:AFF]\"\n",
    "inputs = tokenizer(test_example, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print('Input:', test_example)\n",
    "print('Predicción:', pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
